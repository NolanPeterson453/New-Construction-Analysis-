---
title: "New Constuction Home Analysis"
author: "Nolan Peterson"
date: "1/8/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, comment = NA)
```


## Introduction 

The aims of this analysis are:

* Observe and analyze the trends and relevant statistics for the new construction home market.

* Build a model to predict new home sale prices based off of selected features. 

## Data 

The data for this analysis comes from the Census Bureau's 2020 Survey of Construction. This is a survey of new construction homes across the US which compiles details regarding the construction methods used, physical specifications of the home as well as some legal/financial details surrounding the home. The data can be found here: https://www.census.gov/construction/chars/microdata.html

### Key Insights 

* Raw data has 25,526 observations and 61 variables. 

* Cleaned data has 12,443 observations and 39 variables.

* Cleaned data has 30 categorical predictors, 4 continuous predictors, 1 sample weight variable, and 1 continuous response. 

* Code for data cleaning can be found here: https://github.com/NolanPeterson453/New-Construction-Analysis-/blob/main/01_scripts/data_cleaning.R

### Cleaning Steps

#### Variable Selction

Not all variables in raw data set were useful for this analysis. There were primarily three reasons why variables would be removed. 

* The variable was a flag variable. 

* The variable had low or no predictive power. 

* The variable was proxy for the target response variable. 


For example, the variable SQFS_F was removed because this variable because this variable simply indicates if the Census Bureau has at all modified the reported value. Examples of modification to the reported value include top-coding, rounding or imputation. The second reason for removal (lack of predictive power), was used for variables such as ID or variables describing financing (loan type, etc...). These variables do not add any useful information to this analysis. The third type of variables removed, are variable which describe essentially the same thing thing as the response. That is they describe the value of the home in some measure.

#### Reported Vs Final Observations 

In the raw data set there are several variables which have both reported and 'final' reported counterparts. Take for example the variables SQFS (area of home in square feet) and FSQFS (final area of home in square feet). In cases where the home's final square footage area was different that what was specified in the building permit or contract, the final value is reported in the variable FSQFS. Otherwise the value of FSQFS is 0. In order to simplify the data while still maintaining the accuracy of the reported values the two variables SQFS and FSQFS where merged into a single variable which has the value of FSQFS when it is not 0 and the value of SQFS when FSQFS is 0. 

There is a similar situation for the response SLPR (sale price) and FSLPR (final sale price). FSLPR reports a value when the final sale price is different than what is specified in the contract and is 0 otherwise. These two variables were merged in the exact same way as SQFS and FSQFS. 

#### Missing Values 

There is a substantial amount of unreported values within the data set. For the categorical predictors missing data is simply coded in as an "unreported" or "non-applicable" level in the variable. For continuous variables, unreported values are imputed with the mean value of the reported observations. 

#### Observation Selection

Only observations where the home was sold were kept in the final data set. This is because sale price is the being used as the primary proxy for home value. 

## Exploritory Data Analysis 

### Key Insights 

* Distribution of home sale price is right skewed with the vast majority of homes selling for between \$100,000 and \$750,000. 

### Sale Price of Home 

```{r echo=FALSE}
# Library statements 
library(tidyverse)
library(hablar)
library(scales)
library(ggthemes)
library(ggrepel)
library(knitr)

# Read in data 
soc_data <- read_csv(file = "https://raw.githubusercontent.com/NolanPeterson453/New-Construction-Analysis-/main/02_data/cleaned_soc.csv")

# Assign column types 
factor_vars <- c("ACS", "AGER", "ASSOC", "BASE", "CON", "DECK", "DET", 
                 "DIV", "FNBS", "FOYER", "FRAME", "GAR", "HEAT", "HEAT2", 
                 "LNDR", "MFGS", "PATI", "PRCH", "SEWER", "STOR", "WAL1", 
                 "WAL2", "WALS","WATER", "BEDR", "FPLS", "FULB", "HAFB", "FUEL",
                 "FUEL2")

contin_vars <- c("FSLPR", "FSQFS", "LOTV", "FFNSQ", "AREA")

soc_data <- soc_data %>% 
  convert(fct(factor_vars),
          num(contin_vars))

# Summary statistics for response
summary(soc_data$FSLPR) 
cat(paste0('Mean Sale Price $',round(mean(soc_data$FSLPR), 2), '.\n'))
cat(paste0('Median Sale Price $',round(median(soc_data$FSLPR), 2), '.\n'))

# Distribution of Home Sale Price plot
cuts <- data.frame(ref = c("Mean Sale Price", "Median Sale Price"),
                   vals = c(mean(soc_data$FSLPR), median(soc_data$FSLPR)),
                   stringsAsFactors = FALSE)
soc_data %>% 
  ggplot(aes(x = FSLPR)) +
  geom_bar(stat="bin", 
           fill = "#244747") +
  scale_x_continuous(labels = dollar_format()) +
  labs(title = "Distribution of Home Sale Price",
       x = "Final Sale Price of Home", 
       y = "Count of Homes") + 
  geom_vline(aes(xintercept = vals, color = ref),
             data = cuts,
             show.legend = FALSE) +
  geom_text(aes(x = vals,
                y = c(800,1500), 
                label = ref,
                color = ref,
                angle = 90, 
                vjust = 0.90),
            data = cuts,
            show.legend = FALSE)

```






